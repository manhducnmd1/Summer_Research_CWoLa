{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b65c873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy import interpolate\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1185928d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(5000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 5 seconds\n"
     ]
    }
   ],
   "source": [
    "#Reading files into a log\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "nblog = open(\"nb.log\", \"a+\")\n",
    "sys.stdout.echo = nblog\n",
    "sys.stderr.echo = nblog\n",
    "\n",
    "get_ipython().log.handlers[0].stream = nblog\n",
    "get_ipython().log.setLevel(logging.INFO)\n",
    "\n",
    "%autosave 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59896e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'/home/manhducnmd/pp_dijet/Results_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bc0376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f22518c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d57aaf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CWoLA(p_i):\n",
    "    \n",
    "    input_1 = keras.layers.Input(shape = (p_i,p_i,1), name = \"jet_1\")\n",
    "    y = keras.layers.BatchNormalization()(input_1)\n",
    "    input_2 = keras.layers.Input(shape = (p_i,p_i,1), name = \"jet_2\")\n",
    "    z = keras.layers.BatchNormalization()(input_2)\n",
    "    \n",
    "    x = keras.layers.Concatenate(axis = 0)([y, z])\n",
    "\n",
    "    x = keras.layers.Conv2D(64, (5,5), padding = 'same')(input_1)\n",
    "    x = keras.layers.MaxPool2D(pool_size = (2,2))(x)\n",
    "        \n",
    "    x = keras.layers.Conv2D(64, (5, 5), padding='same')(x)\n",
    "    x = keras.layers.MaxPool2D(pool_size = (2,2))(x)\n",
    "\n",
    "    x = keras.layers.Conv2D(128, (3,3), padding='same')(x)\n",
    "    x = keras.layers.MaxPool2D(pool_size = (2,2))(x)\n",
    "    \n",
    "    x = keras.layers.Conv2D(128, (3,3))(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    \n",
    "    x = keras.layers.Dense(128, activation = 'relu')(x)\n",
    "    x = keras.layers.Dense(128, activation = 'relu')(x)\n",
    "    x = keras.layers.Dense(128, activation = 'relu')(x)\n",
    "    \n",
    "    x = keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
    "    \n",
    "    \n",
    "    return keras.Model(         \n",
    "    inputs=[input_1, input_2],\n",
    "    outputs=x,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5426f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with significance = 0.32, run 0\n",
      "Train on 40762 samples, validate on 10191 samples\n",
      "Epoch 1/1000\n",
      "40762/40762 [==============================] - 2s 45us/sample - loss: 0.7069 - accuracy: 0.4988 - val_loss: 0.6940 - val_accuracy: 0.5012\n",
      "Epoch 2/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6968 - accuracy: 0.5068 - val_loss: 0.6938 - val_accuracy: 0.5085\n",
      "Epoch 3/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6935 - accuracy: 0.5127 - val_loss: 0.6931 - val_accuracy: 0.5072\n",
      "Epoch 4/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6929 - accuracy: 0.5177 - val_loss: 0.7008 - val_accuracy: 0.5076\n",
      "Epoch 5/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6952 - accuracy: 0.5146 - val_loss: 0.6982 - val_accuracy: 0.5020\n",
      "Epoch 6/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6945 - accuracy: 0.5139 - val_loss: 0.6942 - val_accuracy: 0.5121\n",
      "Epoch 7/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6928 - accuracy: 0.5187 - val_loss: 0.6949 - val_accuracy: 0.5049\n",
      "Epoch 8/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6924 - accuracy: 0.5183 - val_loss: 0.6951 - val_accuracy: 0.5099\n",
      "Epoch 9/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6932 - accuracy: 0.5186 - val_loss: 0.6936 - val_accuracy: 0.5052\n",
      "Epoch 10/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6916 - accuracy: 0.5225 - val_loss: 0.6946 - val_accuracy: 0.5084\n",
      "Epoch 11/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6916 - accuracy: 0.5213 - val_loss: 0.6946 - val_accuracy: 0.5098\n",
      "Epoch 12/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6913 - accuracy: 0.5254 - val_loss: 0.6937 - val_accuracy: 0.5068\n",
      "Epoch 13/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6910 - accuracy: 0.5227 - val_loss: 0.6941 - val_accuracy: 0.5088\n",
      "Epoch 14/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6913 - accuracy: 0.5237 - val_loss: 0.6993 - val_accuracy: 0.5075\n",
      "Epoch 15/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6921 - accuracy: 0.5214 - val_loss: 0.6943 - val_accuracy: 0.5023\n",
      "Epoch 16/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6909 - accuracy: 0.5249 - val_loss: 0.6945 - val_accuracy: 0.5075\n",
      "Epoch 17/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6900 - accuracy: 0.5270 - val_loss: 0.6958 - val_accuracy: 0.5025\n",
      "Epoch 18/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6900 - accuracy: 0.5261 - val_loss: 0.6937 - val_accuracy: 0.5064\n",
      "Epoch 19/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6897 - accuracy: 0.5280 - val_loss: 0.6962 - val_accuracy: 0.4982\n",
      "Epoch 20/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6889 - accuracy: 0.5265 - val_loss: 0.6947 - val_accuracy: 0.5095\n",
      "Epoch 21/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6873 - accuracy: 0.5280 - val_loss: 0.6953 - val_accuracy: 0.5049\n",
      "Epoch 22/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6859 - accuracy: 0.5327 - val_loss: 0.6964 - val_accuracy: 0.5044\n",
      "Epoch 23/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6862 - accuracy: 0.5314 - val_loss: 0.6977 - val_accuracy: 0.5098\n",
      "Epoch 24/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6821 - accuracy: 0.5393 - val_loss: 0.7042 - val_accuracy: 0.5061\n",
      "Epoch 25/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6808 - accuracy: 0.5400 - val_loss: 0.7144 - val_accuracy: 0.4922\n",
      "Epoch 26/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6821 - accuracy: 0.5359 - val_loss: 0.7060 - val_accuracy: 0.5031\n",
      "Epoch 27/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6770 - accuracy: 0.5410 - val_loss: 0.7091 - val_accuracy: 0.5039\n",
      "Epoch 28/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6718 - accuracy: 0.5462 - val_loss: 0.7091 - val_accuracy: 0.4998\n",
      "Epoch 29/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6654 - accuracy: 0.5589 - val_loss: 0.7189 - val_accuracy: 0.4963\n",
      "Epoch 30/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6664 - accuracy: 0.5532 - val_loss: 0.7377 - val_accuracy: 0.4929\n",
      "Epoch 31/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6605 - accuracy: 0.5589 - val_loss: 0.7309 - val_accuracy: 0.5075\n",
      "Epoch 32/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6502 - accuracy: 0.5680 - val_loss: 0.7360 - val_accuracy: 0.4999\n",
      "Epoch 33/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6446 - accuracy: 0.5737 - val_loss: 0.7542 - val_accuracy: 0.5028\n",
      "[0.00065    0.01173333 0.13036429]\n",
      "Training with significance = 0.32, run 1\n",
      "Train on 40762 samples, validate on 10191 samples\n",
      "Epoch 1/1000\n",
      "40762/40762 [==============================] - 4s 90us/sample - loss: 0.7068 - accuracy: 0.5021 - val_loss: 0.6966 - val_accuracy: 0.4964\n",
      "Epoch 2/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6974 - accuracy: 0.5062 - val_loss: 0.6952 - val_accuracy: 0.5000\n",
      "Epoch 3/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6944 - accuracy: 0.5127 - val_loss: 0.6949 - val_accuracy: 0.5034\n",
      "Epoch 4/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6934 - accuracy: 0.5121 - val_loss: 0.7094 - val_accuracy: 0.5072\n",
      "Epoch 5/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6955 - accuracy: 0.5149 - val_loss: 0.6976 - val_accuracy: 0.5024\n",
      "Epoch 6/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6958 - accuracy: 0.5077 - val_loss: 0.6955 - val_accuracy: 0.4994\n",
      "Epoch 7/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6929 - accuracy: 0.5168 - val_loss: 0.7010 - val_accuracy: 0.5028\n",
      "Epoch 8/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6928 - accuracy: 0.5169 - val_loss: 0.6946 - val_accuracy: 0.5058\n",
      "Epoch 9/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6932 - accuracy: 0.5194 - val_loss: 0.6938 - val_accuracy: 0.5000\n",
      "Epoch 10/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6914 - accuracy: 0.5266 - val_loss: 0.6942 - val_accuracy: 0.5115\n",
      "Epoch 11/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6919 - accuracy: 0.5204 - val_loss: 0.6953 - val_accuracy: 0.5077\n",
      "Epoch 12/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6914 - accuracy: 0.5259 - val_loss: 0.6954 - val_accuracy: 0.5095\n",
      "Epoch 13/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6926 - accuracy: 0.5205 - val_loss: 0.6934 - val_accuracy: 0.5104\n",
      "Epoch 14/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6913 - accuracy: 0.5221 - val_loss: 0.6972 - val_accuracy: 0.5028\n",
      "Epoch 15/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6914 - accuracy: 0.5248 - val_loss: 0.6937 - val_accuracy: 0.5078\n",
      "Epoch 16/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6899 - accuracy: 0.5267 - val_loss: 0.6948 - val_accuracy: 0.5073\n",
      "Epoch 17/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6918 - accuracy: 0.5218 - val_loss: 0.6952 - val_accuracy: 0.5050\n",
      "Epoch 18/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6902 - accuracy: 0.5258 - val_loss: 0.6941 - val_accuracy: 0.5073\n",
      "Epoch 19/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6906 - accuracy: 0.5258 - val_loss: 0.6957 - val_accuracy: 0.5021\n",
      "Epoch 20/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6904 - accuracy: 0.5270 - val_loss: 0.6952 - val_accuracy: 0.5019\n",
      "Epoch 21/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6878 - accuracy: 0.5304 - val_loss: 0.6961 - val_accuracy: 0.5019\n",
      "Epoch 22/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6872 - accuracy: 0.5329 - val_loss: 0.6972 - val_accuracy: 0.4926\n",
      "Epoch 23/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6866 - accuracy: 0.5320 - val_loss: 0.6985 - val_accuracy: 0.5021\n",
      "Epoch 24/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6858 - accuracy: 0.5337 - val_loss: 0.6955 - val_accuracy: 0.5036\n",
      "Epoch 25/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6841 - accuracy: 0.5387 - val_loss: 0.7012 - val_accuracy: 0.5069\n",
      "Epoch 26/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6837 - accuracy: 0.5377 - val_loss: 0.7046 - val_accuracy: 0.5070\n",
      "Epoch 27/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6820 - accuracy: 0.5397 - val_loss: 0.7002 - val_accuracy: 0.5052\n",
      "Epoch 28/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6773 - accuracy: 0.5438 - val_loss: 0.7026 - val_accuracy: 0.5088\n",
      "Epoch 29/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6743 - accuracy: 0.5507 - val_loss: 0.7162 - val_accuracy: 0.4969\n",
      "Epoch 30/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6731 - accuracy: 0.5478 - val_loss: 0.7186 - val_accuracy: 0.4951\n",
      "Epoch 31/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6687 - accuracy: 0.5545 - val_loss: 0.7085 - val_accuracy: 0.5025\n",
      "Epoch 32/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6628 - accuracy: 0.5610 - val_loss: 0.7213 - val_accuracy: 0.5016\n",
      "Epoch 33/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6586 - accuracy: 0.5667 - val_loss: 0.7335 - val_accuracy: 0.4954\n",
      "Epoch 34/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6514 - accuracy: 0.5708 - val_loss: 0.7273 - val_accuracy: 0.5014\n",
      "Epoch 35/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6478 - accuracy: 0.5726 - val_loss: 0.7432 - val_accuracy: 0.5067\n",
      "Epoch 36/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6417 - accuracy: 0.5771 - val_loss: 0.7377 - val_accuracy: 0.5014\n",
      "Epoch 37/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6351 - accuracy: 0.5825 - val_loss: 0.7504 - val_accuracy: 0.4982\n",
      "Epoch 38/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6310 - accuracy: 0.5852 - val_loss: 0.7674 - val_accuracy: 0.5045\n",
      "Epoch 39/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6180 - accuracy: 0.5909 - val_loss: 0.7844 - val_accuracy: 0.5039\n",
      "Epoch 40/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6118 - accuracy: 0.5970 - val_loss: 0.8128 - val_accuracy: 0.5013\n",
      "Epoch 41/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6115 - accuracy: 0.5965 - val_loss: 0.8048 - val_accuracy: 0.5007\n",
      "Epoch 42/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6011 - accuracy: 0.6038 - val_loss: 0.7985 - val_accuracy: 0.5044\n",
      "Epoch 43/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.5967 - accuracy: 0.6077 - val_loss: 0.8351 - val_accuracy: 0.5001\n",
      "[0.00045 0.0122  0.13155]\n",
      "Training with significance = 0.32, run 2\n",
      "Train on 40762 samples, validate on 10191 samples\n",
      "Epoch 1/1000\n",
      "40762/40762 [==============================] - 2s 47us/sample - loss: 0.7229 - accuracy: 0.5033 - val_loss: 0.7001 - val_accuracy: 0.4994\n",
      "Epoch 2/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6982 - accuracy: 0.5057 - val_loss: 0.6937 - val_accuracy: 0.5040\n",
      "Epoch 3/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6941 - accuracy: 0.5140 - val_loss: 0.6945 - val_accuracy: 0.5086\n",
      "Epoch 4/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6931 - accuracy: 0.5188 - val_loss: 0.7079 - val_accuracy: 0.5076\n",
      "Epoch 5/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6977 - accuracy: 0.5145 - val_loss: 0.6956 - val_accuracy: 0.5026\n",
      "Epoch 6/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6939 - accuracy: 0.5163 - val_loss: 0.6939 - val_accuracy: 0.5121\n",
      "Epoch 7/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6930 - accuracy: 0.5218 - val_loss: 0.7031 - val_accuracy: 0.4984\n",
      "Epoch 8/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6924 - accuracy: 0.5212 - val_loss: 0.6935 - val_accuracy: 0.5105\n",
      "Epoch 9/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6935 - accuracy: 0.5207 - val_loss: 0.6945 - val_accuracy: 0.5104\n",
      "Epoch 10/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6914 - accuracy: 0.5238 - val_loss: 0.6939 - val_accuracy: 0.5105\n",
      "Epoch 11/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6922 - accuracy: 0.5235 - val_loss: 0.6939 - val_accuracy: 0.5062\n",
      "Epoch 12/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6916 - accuracy: 0.5254 - val_loss: 0.6958 - val_accuracy: 0.5099\n",
      "Epoch 13/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6914 - accuracy: 0.5228 - val_loss: 0.6944 - val_accuracy: 0.5032\n",
      "Epoch 14/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6907 - accuracy: 0.5268 - val_loss: 0.7066 - val_accuracy: 0.5042\n",
      "Epoch 15/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6904 - accuracy: 0.5290 - val_loss: 0.6938 - val_accuracy: 0.5055\n",
      "Epoch 16/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6900 - accuracy: 0.5297 - val_loss: 0.6963 - val_accuracy: 0.5026\n",
      "Epoch 17/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6919 - accuracy: 0.5237 - val_loss: 0.6947 - val_accuracy: 0.5123\n",
      "Epoch 18/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6910 - accuracy: 0.5258 - val_loss: 0.6943 - val_accuracy: 0.5039\n",
      "Epoch 19/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6894 - accuracy: 0.5268 - val_loss: 0.6948 - val_accuracy: 0.5034\n",
      "Epoch 20/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6893 - accuracy: 0.5273 - val_loss: 0.6965 - val_accuracy: 0.5052\n",
      "Epoch 21/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6884 - accuracy: 0.5313 - val_loss: 0.6954 - val_accuracy: 0.5100\n",
      "Epoch 22/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6871 - accuracy: 0.5332 - val_loss: 0.6960 - val_accuracy: 0.4986\n",
      "Epoch 23/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6865 - accuracy: 0.5306 - val_loss: 0.6962 - val_accuracy: 0.4997\n",
      "Epoch 24/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6849 - accuracy: 0.5414 - val_loss: 0.6986 - val_accuracy: 0.5055\n",
      "Epoch 25/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6835 - accuracy: 0.5357 - val_loss: 0.6984 - val_accuracy: 0.5017\n",
      "Epoch 26/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6835 - accuracy: 0.5369 - val_loss: 0.6996 - val_accuracy: 0.4974\n",
      "Epoch 27/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6816 - accuracy: 0.5380 - val_loss: 0.7011 - val_accuracy: 0.5005\n",
      "Epoch 28/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6795 - accuracy: 0.5434 - val_loss: 0.7021 - val_accuracy: 0.5128\n",
      "Epoch 29/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6748 - accuracy: 0.5493 - val_loss: 0.7066 - val_accuracy: 0.5059\n",
      "Epoch 30/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6739 - accuracy: 0.5503 - val_loss: 0.7054 - val_accuracy: 0.5017\n",
      "Epoch 31/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6698 - accuracy: 0.5554 - val_loss: 0.7085 - val_accuracy: 0.5040\n",
      "Epoch 32/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6625 - accuracy: 0.5625 - val_loss: 0.7196 - val_accuracy: 0.4957\n",
      "Epoch 33/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6582 - accuracy: 0.5640 - val_loss: 0.7258 - val_accuracy: 0.4994\n",
      "Epoch 34/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6522 - accuracy: 0.5671 - val_loss: 0.7361 - val_accuracy: 0.5030\n",
      "Epoch 35/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6480 - accuracy: 0.5729 - val_loss: 0.7262 - val_accuracy: 0.5051\n",
      "Epoch 36/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6463 - accuracy: 0.5722 - val_loss: 0.7446 - val_accuracy: 0.5036\n",
      "Epoch 37/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6385 - accuracy: 0.5764 - val_loss: 0.7614 - val_accuracy: 0.5028\n",
      "Epoch 38/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6303 - accuracy: 0.5861 - val_loss: 0.7513 - val_accuracy: 0.5051\n",
      "[0.000825   0.00875    0.08007143]\n",
      "Training with significance = 0.32, run 3\n",
      "Train on 40762 samples, validate on 10191 samples\n",
      "Epoch 1/1000\n",
      "40762/40762 [==============================] - 2s 46us/sample - loss: 0.7040 - accuracy: 0.5032 - val_loss: 0.6994 - val_accuracy: 0.5000\n",
      "Epoch 2/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6956 - accuracy: 0.5159 - val_loss: 0.6937 - val_accuracy: 0.5100\n",
      "Epoch 3/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6958 - accuracy: 0.5110 - val_loss: 0.6955 - val_accuracy: 0.5050\n",
      "Epoch 4/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6945 - accuracy: 0.5132 - val_loss: 0.7076 - val_accuracy: 0.5075\n",
      "Epoch 5/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6952 - accuracy: 0.5143 - val_loss: 0.6974 - val_accuracy: 0.4959\n",
      "Epoch 6/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6947 - accuracy: 0.5098 - val_loss: 0.6934 - val_accuracy: 0.5108\n",
      "Epoch 7/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6925 - accuracy: 0.5193 - val_loss: 0.6958 - val_accuracy: 0.5065\n",
      "Epoch 8/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6926 - accuracy: 0.5201 - val_loss: 0.6951 - val_accuracy: 0.5076\n",
      "Epoch 9/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6935 - accuracy: 0.5190 - val_loss: 0.6945 - val_accuracy: 0.4996\n",
      "Epoch 10/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6929 - accuracy: 0.5192 - val_loss: 0.6950 - val_accuracy: 0.5081\n",
      "Epoch 11/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6926 - accuracy: 0.5214 - val_loss: 0.6938 - val_accuracy: 0.5114\n",
      "Epoch 12/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6924 - accuracy: 0.5211 - val_loss: 0.6949 - val_accuracy: 0.5050\n",
      "Epoch 13/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6933 - accuracy: 0.5162 - val_loss: 0.6941 - val_accuracy: 0.5055\n",
      "Epoch 14/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6921 - accuracy: 0.5190 - val_loss: 0.6994 - val_accuracy: 0.5019\n",
      "Epoch 15/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6927 - accuracy: 0.5216 - val_loss: 0.6940 - val_accuracy: 0.5075\n",
      "Epoch 16/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6915 - accuracy: 0.5245 - val_loss: 0.6943 - val_accuracy: 0.5035\n",
      "Epoch 17/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6915 - accuracy: 0.5255 - val_loss: 0.6948 - val_accuracy: 0.5022\n",
      "Epoch 18/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6913 - accuracy: 0.5229 - val_loss: 0.6945 - val_accuracy: 0.5056\n",
      "Epoch 19/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6907 - accuracy: 0.5209 - val_loss: 0.6955 - val_accuracy: 0.5029\n",
      "Epoch 20/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6905 - accuracy: 0.5263 - val_loss: 0.6940 - val_accuracy: 0.5055\n",
      "Epoch 21/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6892 - accuracy: 0.5278 - val_loss: 0.6945 - val_accuracy: 0.5045\n",
      "Epoch 22/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6891 - accuracy: 0.5284 - val_loss: 0.7064 - val_accuracy: 0.5042\n",
      "Epoch 23/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6901 - accuracy: 0.5281 - val_loss: 0.6968 - val_accuracy: 0.5007\n",
      "Epoch 24/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6887 - accuracy: 0.5300 - val_loss: 0.7040 - val_accuracy: 0.4937\n",
      "Epoch 25/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6887 - accuracy: 0.5285 - val_loss: 0.6977 - val_accuracy: 0.5052\n",
      "Epoch 26/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6879 - accuracy: 0.5287 - val_loss: 0.6997 - val_accuracy: 0.4995\n",
      "Epoch 27/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6871 - accuracy: 0.5307 - val_loss: 0.6974 - val_accuracy: 0.4966\n",
      "Epoch 28/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6838 - accuracy: 0.5346 - val_loss: 0.6996 - val_accuracy: 0.5046\n",
      "Epoch 29/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6815 - accuracy: 0.5398 - val_loss: 0.7049 - val_accuracy: 0.5013\n",
      "Epoch 30/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6808 - accuracy: 0.5393 - val_loss: 0.7054 - val_accuracy: 0.5061\n",
      "Epoch 31/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6783 - accuracy: 0.5429 - val_loss: 0.7073 - val_accuracy: 0.5065\n",
      "Epoch 32/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6742 - accuracy: 0.5471 - val_loss: 0.7067 - val_accuracy: 0.5042\n",
      "Epoch 33/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6708 - accuracy: 0.5485 - val_loss: 0.7080 - val_accuracy: 0.5078\n",
      "Epoch 34/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6679 - accuracy: 0.5526 - val_loss: 0.7024 - val_accuracy: 0.5053\n",
      "Epoch 35/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6653 - accuracy: 0.5587 - val_loss: 0.7088 - val_accuracy: 0.5118\n",
      "Epoch 36/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6621 - accuracy: 0.5599 - val_loss: 0.7159 - val_accuracy: 0.5046\n",
      "[0.00055 0.0086  0.09905]\n",
      "Training with significance = 0.32, run 4\n",
      "Train on 40762 samples, validate on 10191 samples\n",
      "Epoch 1/1000\n",
      "40762/40762 [==============================] - 2s 46us/sample - loss: 0.7151 - accuracy: 0.5022 - val_loss: 0.6956 - val_accuracy: 0.5016\n",
      "Epoch 2/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6960 - accuracy: 0.5060 - val_loss: 0.6943 - val_accuracy: 0.5038\n",
      "Epoch 3/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6936 - accuracy: 0.5125 - val_loss: 0.6940 - val_accuracy: 0.5039\n",
      "Epoch 4/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6936 - accuracy: 0.5116 - val_loss: 0.7003 - val_accuracy: 0.5063\n",
      "Epoch 5/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6962 - accuracy: 0.5144 - val_loss: 0.6950 - val_accuracy: 0.4994\n",
      "Epoch 6/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6931 - accuracy: 0.5134 - val_loss: 0.6962 - val_accuracy: 0.5041\n",
      "Epoch 7/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6927 - accuracy: 0.5158 - val_loss: 0.6949 - val_accuracy: 0.5064\n",
      "Epoch 8/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6925 - accuracy: 0.5195 - val_loss: 0.6944 - val_accuracy: 0.5091\n",
      "Epoch 9/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6929 - accuracy: 0.5181 - val_loss: 0.6946 - val_accuracy: 0.5052\n",
      "Epoch 10/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6918 - accuracy: 0.5234 - val_loss: 0.6957 - val_accuracy: 0.5066\n",
      "Epoch 11/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6919 - accuracy: 0.5221 - val_loss: 0.6948 - val_accuracy: 0.5085\n",
      "Epoch 12/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6919 - accuracy: 0.5208 - val_loss: 0.6958 - val_accuracy: 0.5068\n",
      "Epoch 13/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6921 - accuracy: 0.5169 - val_loss: 0.6949 - val_accuracy: 0.5042\n",
      "Epoch 14/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6920 - accuracy: 0.5204 - val_loss: 0.7062 - val_accuracy: 0.5043\n",
      "Epoch 15/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6918 - accuracy: 0.5235 - val_loss: 0.6942 - val_accuracy: 0.5049\n",
      "Epoch 16/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6915 - accuracy: 0.5202 - val_loss: 0.6952 - val_accuracy: 0.5064\n",
      "Epoch 17/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6918 - accuracy: 0.5218 - val_loss: 0.6948 - val_accuracy: 0.5094\n",
      "Epoch 18/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6921 - accuracy: 0.5193 - val_loss: 0.6939 - val_accuracy: 0.5075\n",
      "Epoch 19/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6909 - accuracy: 0.5227 - val_loss: 0.6975 - val_accuracy: 0.5024\n",
      "Epoch 20/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6901 - accuracy: 0.5237 - val_loss: 0.6982 - val_accuracy: 0.5050\n",
      "Epoch 21/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6899 - accuracy: 0.5228 - val_loss: 0.6952 - val_accuracy: 0.5054\n",
      "Epoch 22/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6912 - accuracy: 0.5231 - val_loss: 0.7001 - val_accuracy: 0.4954\n",
      "Epoch 23/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6891 - accuracy: 0.5286 - val_loss: 0.6949 - val_accuracy: 0.5042\n",
      "Epoch 24/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6876 - accuracy: 0.5306 - val_loss: 0.6987 - val_accuracy: 0.5004\n",
      "Epoch 25/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6876 - accuracy: 0.5294 - val_loss: 0.6952 - val_accuracy: 0.5044\n",
      "Epoch 26/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6874 - accuracy: 0.5272 - val_loss: 0.6992 - val_accuracy: 0.5011\n",
      "Epoch 27/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6844 - accuracy: 0.5330 - val_loss: 0.6963 - val_accuracy: 0.5030\n",
      "Epoch 28/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6820 - accuracy: 0.5396 - val_loss: 0.6983 - val_accuracy: 0.5034\n",
      "Epoch 29/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6809 - accuracy: 0.5359 - val_loss: 0.7019 - val_accuracy: 0.5045\n",
      "Epoch 30/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6784 - accuracy: 0.5409 - val_loss: 0.6996 - val_accuracy: 0.5033\n",
      "Epoch 31/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6728 - accuracy: 0.5452 - val_loss: 0.7116 - val_accuracy: 0.4970\n",
      "Epoch 32/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6682 - accuracy: 0.5560 - val_loss: 0.7111 - val_accuracy: 0.5052\n",
      "Epoch 33/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6631 - accuracy: 0.5577 - val_loss: 0.7267 - val_accuracy: 0.4958\n",
      "Epoch 34/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6601 - accuracy: 0.5586 - val_loss: 0.7273 - val_accuracy: 0.5017\n",
      "Epoch 35/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6560 - accuracy: 0.5643 - val_loss: 0.7330 - val_accuracy: 0.5000\n",
      "Epoch 36/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6545 - accuracy: 0.5632 - val_loss: 0.7485 - val_accuracy: 0.5035\n",
      "Epoch 37/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6447 - accuracy: 0.5748 - val_loss: 0.7459 - val_accuracy: 0.5039\n",
      "Epoch 38/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6355 - accuracy: 0.5796 - val_loss: 0.7476 - val_accuracy: 0.5010\n",
      "Epoch 39/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6266 - accuracy: 0.5847 - val_loss: 0.7807 - val_accuracy: 0.5030\n",
      "Epoch 40/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6246 - accuracy: 0.5849 - val_loss: 0.7809 - val_accuracy: 0.5030\n",
      "Epoch 41/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6181 - accuracy: 0.5903 - val_loss: 0.7849 - val_accuracy: 0.5080\n",
      "Epoch 42/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6118 - accuracy: 0.5961 - val_loss: 0.8070 - val_accuracy: 0.5052\n",
      "Epoch 43/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6096 - accuracy: 0.5969 - val_loss: 0.8234 - val_accuracy: 0.4985\n",
      "Epoch 44/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6018 - accuracy: 0.5998 - val_loss: 0.8374 - val_accuracy: 0.5055\n",
      "Epoch 45/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.5980 - accuracy: 0.6031 - val_loss: 0.8725 - val_accuracy: 0.5067\n",
      "Epoch 46/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.5921 - accuracy: 0.6060 - val_loss: 0.8873 - val_accuracy: 0.5033\n",
      "Epoch 47/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.5929 - accuracy: 0.6084 - val_loss: 0.8756 - val_accuracy: 0.5003\n",
      "Epoch 48/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.5821 - accuracy: 0.6160 - val_loss: 0.9377 - val_accuracy: 0.5025\n",
      "[0.0003  0.00725 0.12645]\n",
      "Training with significance = 0.32, run 5\n",
      "Train on 40762 samples, validate on 10191 samples\n",
      "Epoch 1/1000\n",
      "40762/40762 [==============================] - 2s 46us/sample - loss: 0.7085 - accuracy: 0.5040 - val_loss: 0.6977 - val_accuracy: 0.5004\n",
      "Epoch 2/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6964 - accuracy: 0.5097 - val_loss: 0.6938 - val_accuracy: 0.5083\n",
      "Epoch 3/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6951 - accuracy: 0.5095 - val_loss: 0.6952 - val_accuracy: 0.5057\n",
      "Epoch 4/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6942 - accuracy: 0.5149 - val_loss: 0.7125 - val_accuracy: 0.5071\n",
      "Epoch 5/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6956 - accuracy: 0.5167 - val_loss: 0.7015 - val_accuracy: 0.5001\n",
      "Epoch 6/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6977 - accuracy: 0.5093 - val_loss: 0.6951 - val_accuracy: 0.5040\n",
      "Epoch 7/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6950 - accuracy: 0.5114 - val_loss: 0.6956 - val_accuracy: 0.5064\n",
      "Epoch 8/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6936 - accuracy: 0.5186 - val_loss: 0.6940 - val_accuracy: 0.5054\n",
      "Epoch 9/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6939 - accuracy: 0.5165 - val_loss: 0.6943 - val_accuracy: 0.4998\n",
      "Epoch 10/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6931 - accuracy: 0.5164 - val_loss: 0.6940 - val_accuracy: 0.5078\n",
      "Epoch 11/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6925 - accuracy: 0.5209 - val_loss: 0.6956 - val_accuracy: 0.5040\n",
      "Epoch 12/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6926 - accuracy: 0.5192 - val_loss: 0.6954 - val_accuracy: 0.5065\n",
      "Epoch 13/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6923 - accuracy: 0.5204 - val_loss: 0.6946 - val_accuracy: 0.5031\n",
      "Epoch 14/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6917 - accuracy: 0.5234 - val_loss: 0.6991 - val_accuracy: 0.5012\n",
      "Epoch 15/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6924 - accuracy: 0.5210 - val_loss: 0.6937 - val_accuracy: 0.5027\n",
      "Epoch 16/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6918 - accuracy: 0.5226 - val_loss: 0.6951 - val_accuracy: 0.5043\n",
      "Epoch 17/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6919 - accuracy: 0.5206 - val_loss: 0.6953 - val_accuracy: 0.5046\n",
      "Epoch 18/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6918 - accuracy: 0.5218 - val_loss: 0.6946 - val_accuracy: 0.5038\n",
      "Epoch 19/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6908 - accuracy: 0.5268 - val_loss: 0.6959 - val_accuracy: 0.5006\n",
      "Epoch 20/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6905 - accuracy: 0.5257 - val_loss: 0.6964 - val_accuracy: 0.4999\n",
      "Epoch 21/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6893 - accuracy: 0.5258 - val_loss: 0.6950 - val_accuracy: 0.5049\n",
      "Epoch 22/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6885 - accuracy: 0.5297 - val_loss: 0.6963 - val_accuracy: 0.5006\n",
      "Epoch 23/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6875 - accuracy: 0.5343 - val_loss: 0.6962 - val_accuracy: 0.5025\n",
      "Epoch 24/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6872 - accuracy: 0.5307 - val_loss: 0.7004 - val_accuracy: 0.4971\n",
      "Epoch 25/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6862 - accuracy: 0.5341 - val_loss: 0.6962 - val_accuracy: 0.5082\n",
      "Epoch 26/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6851 - accuracy: 0.5341 - val_loss: 0.6981 - val_accuracy: 0.5085\n",
      "Epoch 27/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6849 - accuracy: 0.5305 - val_loss: 0.6994 - val_accuracy: 0.5006\n",
      "Epoch 28/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6822 - accuracy: 0.5358 - val_loss: 0.6991 - val_accuracy: 0.5115\n",
      "Epoch 29/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6795 - accuracy: 0.5427 - val_loss: 0.7065 - val_accuracy: 0.4977\n",
      "Epoch 30/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6776 - accuracy: 0.5446 - val_loss: 0.7079 - val_accuracy: 0.5057\n",
      "Epoch 31/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6728 - accuracy: 0.5525 - val_loss: 0.7139 - val_accuracy: 0.4984\n",
      "Epoch 32/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6683 - accuracy: 0.5543 - val_loss: 0.7273 - val_accuracy: 0.4964\n",
      "Epoch 33/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6654 - accuracy: 0.5538 - val_loss: 0.7226 - val_accuracy: 0.5032\n",
      "Epoch 34/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6613 - accuracy: 0.5591 - val_loss: 0.7288 - val_accuracy: 0.5052\n",
      "Epoch 35/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6574 - accuracy: 0.5647 - val_loss: 0.7302 - val_accuracy: 0.5005\n",
      "Epoch 36/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6492 - accuracy: 0.5712 - val_loss: 0.7556 - val_accuracy: 0.5073\n",
      "Epoch 37/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6467 - accuracy: 0.5712 - val_loss: 0.7485 - val_accuracy: 0.4946\n",
      "Epoch 38/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6357 - accuracy: 0.5805 - val_loss: 0.7525 - val_accuracy: 0.4996\n",
      "Epoch 39/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6287 - accuracy: 0.5819 - val_loss: 0.7789 - val_accuracy: 0.5031\n",
      "Epoch 40/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6218 - accuracy: 0.5932 - val_loss: 0.8076 - val_accuracy: 0.4990\n",
      "Epoch 41/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6182 - accuracy: 0.5932 - val_loss: 0.7825 - val_accuracy: 0.5010\n",
      "Epoch 42/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6174 - accuracy: 0.5951 - val_loss: 0.7993 - val_accuracy: 0.5073\n",
      "Epoch 43/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6077 - accuracy: 0.6008 - val_loss: 0.8179 - val_accuracy: 0.4984\n",
      "Epoch 44/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6005 - accuracy: 0.6084 - val_loss: 0.8399 - val_accuracy: 0.5077\n",
      "Epoch 45/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.5945 - accuracy: 0.6066 - val_loss: 0.8581 - val_accuracy: 0.4999\n",
      "[0.0009 0.0219 0.1629]\n",
      "Training with significance = 0.32, run 6\n",
      "Train on 40762 samples, validate on 10191 samples\n",
      "Epoch 1/1000\n",
      "40762/40762 [==============================] - 2s 46us/sample - loss: 0.7109 - accuracy: 0.4968 - val_loss: 0.6951 - val_accuracy: 0.5091\n",
      "Epoch 2/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6952 - accuracy: 0.5041 - val_loss: 0.6941 - val_accuracy: 0.5026\n",
      "Epoch 3/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6937 - accuracy: 0.5104 - val_loss: 0.6941 - val_accuracy: 0.5053\n",
      "Epoch 4/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6928 - accuracy: 0.5133 - val_loss: 0.6975 - val_accuracy: 0.5070\n",
      "Epoch 5/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6959 - accuracy: 0.5107 - val_loss: 0.6942 - val_accuracy: 0.5066\n",
      "Epoch 6/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6948 - accuracy: 0.5111 - val_loss: 0.6982 - val_accuracy: 0.4918\n",
      "Epoch 7/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6931 - accuracy: 0.5146 - val_loss: 0.7001 - val_accuracy: 0.5019\n",
      "Epoch 8/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6933 - accuracy: 0.5131 - val_loss: 0.6947 - val_accuracy: 0.5032\n",
      "Epoch 9/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6936 - accuracy: 0.5154 - val_loss: 0.6954 - val_accuracy: 0.5049\n",
      "Epoch 10/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6927 - accuracy: 0.5186 - val_loss: 0.6941 - val_accuracy: 0.5090\n",
      "Epoch 11/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6924 - accuracy: 0.5174 - val_loss: 0.6955 - val_accuracy: 0.5029\n",
      "Epoch 12/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6918 - accuracy: 0.5194 - val_loss: 0.6955 - val_accuracy: 0.5079\n",
      "Epoch 13/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6924 - accuracy: 0.5207 - val_loss: 0.6941 - val_accuracy: 0.5011\n",
      "Epoch 14/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6919 - accuracy: 0.5236 - val_loss: 0.6971 - val_accuracy: 0.5031\n",
      "Epoch 15/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6914 - accuracy: 0.5248 - val_loss: 0.6951 - val_accuracy: 0.5000\n",
      "Epoch 16/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6919 - accuracy: 0.5230 - val_loss: 0.6971 - val_accuracy: 0.5065\n",
      "Epoch 17/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6923 - accuracy: 0.5237 - val_loss: 0.6945 - val_accuracy: 0.5054\n",
      "Epoch 18/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6920 - accuracy: 0.5210 - val_loss: 0.6944 - val_accuracy: 0.5064\n",
      "Epoch 19/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6911 - accuracy: 0.5205 - val_loss: 0.6969 - val_accuracy: 0.4949\n",
      "Epoch 20/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6899 - accuracy: 0.5270 - val_loss: 0.6969 - val_accuracy: 0.5021\n",
      "Epoch 21/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6896 - accuracy: 0.5247 - val_loss: 0.6960 - val_accuracy: 0.5033\n",
      "Epoch 22/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6900 - accuracy: 0.5238 - val_loss: 0.6999 - val_accuracy: 0.5040\n",
      "Epoch 23/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6888 - accuracy: 0.5292 - val_loss: 0.6977 - val_accuracy: 0.4952\n",
      "Epoch 24/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6878 - accuracy: 0.5302 - val_loss: 0.7001 - val_accuracy: 0.4979\n",
      "Epoch 25/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6856 - accuracy: 0.5353 - val_loss: 0.6986 - val_accuracy: 0.5019\n",
      "Epoch 26/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6841 - accuracy: 0.5366 - val_loss: 0.6988 - val_accuracy: 0.5000\n",
      "Epoch 27/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6824 - accuracy: 0.5393 - val_loss: 0.7020 - val_accuracy: 0.5033\n",
      "Epoch 28/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6804 - accuracy: 0.5413 - val_loss: 0.7021 - val_accuracy: 0.4979\n",
      "Epoch 29/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6763 - accuracy: 0.5467 - val_loss: 0.7074 - val_accuracy: 0.4992\n",
      "Epoch 30/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6755 - accuracy: 0.5440 - val_loss: 0.7095 - val_accuracy: 0.5093\n",
      "Epoch 31/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6710 - accuracy: 0.5490 - val_loss: 0.7126 - val_accuracy: 0.5017\n",
      "Epoch 32/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6654 - accuracy: 0.5588 - val_loss: 0.7113 - val_accuracy: 0.5024\n",
      "Epoch 33/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6625 - accuracy: 0.5610 - val_loss: 0.7208 - val_accuracy: 0.5017\n",
      "Epoch 34/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6562 - accuracy: 0.5669 - val_loss: 0.7247 - val_accuracy: 0.5060\n",
      "Epoch 35/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6556 - accuracy: 0.5646 - val_loss: 0.7196 - val_accuracy: 0.5076\n",
      "Epoch 36/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6497 - accuracy: 0.5703 - val_loss: 0.7351 - val_accuracy: 0.5029\n",
      "Epoch 37/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6427 - accuracy: 0.5741 - val_loss: 0.7588 - val_accuracy: 0.4977\n",
      "Epoch 38/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6339 - accuracy: 0.5792 - val_loss: 0.7529 - val_accuracy: 0.5054\n",
      "Epoch 39/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6256 - accuracy: 0.5885 - val_loss: 0.7984 - val_accuracy: 0.4997\n",
      "Epoch 40/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6235 - accuracy: 0.5920 - val_loss: 0.7960 - val_accuracy: 0.5031\n",
      "Epoch 41/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6192 - accuracy: 0.5906 - val_loss: 0.7791 - val_accuracy: 0.5098\n",
      "Epoch 42/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6090 - accuracy: 0.5983 - val_loss: 0.8065 - val_accuracy: 0.5060\n",
      "Epoch 43/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6014 - accuracy: 0.6018 - val_loss: 0.8274 - val_accuracy: 0.5021\n",
      "[0.00215    0.03365    0.18763333]\n",
      "Training with significance = 0.32, run 7\n",
      "Train on 40762 samples, validate on 10191 samples\n",
      "Epoch 1/1000\n",
      "40762/40762 [==============================] - 2s 45us/sample - loss: 0.7213 - accuracy: 0.5029 - val_loss: 0.6938 - val_accuracy: 0.5066\n",
      "Epoch 2/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6952 - accuracy: 0.5098 - val_loss: 0.6938 - val_accuracy: 0.5078\n",
      "Epoch 3/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6936 - accuracy: 0.5154 - val_loss: 0.6942 - val_accuracy: 0.5078\n",
      "Epoch 4/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6926 - accuracy: 0.5178 - val_loss: 0.6997 - val_accuracy: 0.5092\n",
      "Epoch 5/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6960 - accuracy: 0.5122 - val_loss: 0.6952 - val_accuracy: 0.5081\n",
      "Epoch 6/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6926 - accuracy: 0.5153 - val_loss: 0.6938 - val_accuracy: 0.5070\n",
      "Epoch 7/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6921 - accuracy: 0.5239 - val_loss: 0.6942 - val_accuracy: 0.5050\n",
      "Epoch 8/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6919 - accuracy: 0.5199 - val_loss: 0.6948 - val_accuracy: 0.5080\n",
      "Epoch 9/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6939 - accuracy: 0.5188 - val_loss: 0.6936 - val_accuracy: 0.5058\n",
      "Epoch 10/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6919 - accuracy: 0.5210 - val_loss: 0.6940 - val_accuracy: 0.5093\n",
      "Epoch 11/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6924 - accuracy: 0.5210 - val_loss: 0.6940 - val_accuracy: 0.5058\n",
      "Epoch 12/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6916 - accuracy: 0.5239 - val_loss: 0.6934 - val_accuracy: 0.5070\n",
      "Epoch 13/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6919 - accuracy: 0.5209 - val_loss: 0.7167 - val_accuracy: 0.5050\n",
      "Epoch 14/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6924 - accuracy: 0.5201 - val_loss: 0.6979 - val_accuracy: 0.5038\n",
      "Epoch 15/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6919 - accuracy: 0.5236 - val_loss: 0.6939 - val_accuracy: 0.5062\n",
      "Epoch 16/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6920 - accuracy: 0.5215 - val_loss: 0.6951 - val_accuracy: 0.5061\n",
      "Epoch 17/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6915 - accuracy: 0.5259 - val_loss: 0.6945 - val_accuracy: 0.5045\n",
      "Epoch 18/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6908 - accuracy: 0.5261 - val_loss: 0.6938 - val_accuracy: 0.5070\n",
      "Epoch 19/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6900 - accuracy: 0.5255 - val_loss: 0.6965 - val_accuracy: 0.5028\n",
      "Epoch 20/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6899 - accuracy: 0.5268 - val_loss: 0.6944 - val_accuracy: 0.5106\n",
      "Epoch 21/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6900 - accuracy: 0.5292 - val_loss: 0.6947 - val_accuracy: 0.5075\n",
      "Epoch 22/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6903 - accuracy: 0.5250 - val_loss: 0.7019 - val_accuracy: 0.5038\n",
      "Epoch 23/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6896 - accuracy: 0.5312 - val_loss: 0.6984 - val_accuracy: 0.5058\n",
      "Epoch 24/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6878 - accuracy: 0.5352 - val_loss: 0.7010 - val_accuracy: 0.4984\n",
      "Epoch 25/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6872 - accuracy: 0.5321 - val_loss: 0.6973 - val_accuracy: 0.5103\n",
      "Epoch 26/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6859 - accuracy: 0.5352 - val_loss: 0.6968 - val_accuracy: 0.5051\n",
      "Epoch 27/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6871 - accuracy: 0.5327 - val_loss: 0.6982 - val_accuracy: 0.4942\n",
      "Epoch 28/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6836 - accuracy: 0.5387 - val_loss: 0.6978 - val_accuracy: 0.5094\n",
      "Epoch 29/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6823 - accuracy: 0.5411 - val_loss: 0.6982 - val_accuracy: 0.5058\n",
      "Epoch 30/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6809 - accuracy: 0.5395 - val_loss: 0.7036 - val_accuracy: 0.5004\n",
      "Epoch 31/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6771 - accuracy: 0.5470 - val_loss: 0.7028 - val_accuracy: 0.4989\n",
      "Epoch 32/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6747 - accuracy: 0.5503 - val_loss: 0.7082 - val_accuracy: 0.4975\n",
      "Epoch 33/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6722 - accuracy: 0.5494 - val_loss: 0.7086 - val_accuracy: 0.4983\n",
      "Epoch 34/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6682 - accuracy: 0.5547 - val_loss: 0.7109 - val_accuracy: 0.4989\n",
      "Epoch 35/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6645 - accuracy: 0.5560 - val_loss: 0.7128 - val_accuracy: 0.5027\n",
      "Epoch 36/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6626 - accuracy: 0.5593 - val_loss: 0.7220 - val_accuracy: 0.5058\n",
      "Epoch 37/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6569 - accuracy: 0.5632 - val_loss: 0.7276 - val_accuracy: 0.5067\n",
      "Epoch 38/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6512 - accuracy: 0.5712 - val_loss: 0.7409 - val_accuracy: 0.5067\n",
      "Epoch 39/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6459 - accuracy: 0.5712 - val_loss: 0.7347 - val_accuracy: 0.4978\n",
      "Epoch 40/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6379 - accuracy: 0.5780 - val_loss: 0.7430 - val_accuracy: 0.5047\n",
      "Epoch 41/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6323 - accuracy: 0.5845 - val_loss: 0.7550 - val_accuracy: 0.5032\n",
      "Epoch 42/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6258 - accuracy: 0.5862 - val_loss: 0.7649 - val_accuracy: 0.4986\n",
      "[0.00065  0.017625 0.15205 ]\n",
      "Training with significance = 0.32, run 8\n",
      "Train on 40762 samples, validate on 10191 samples\n",
      "Epoch 1/1000\n",
      "40762/40762 [==============================] - 2s 44us/sample - loss: 0.7142 - accuracy: 0.5077 - val_loss: 0.6965 - val_accuracy: 0.4986\n",
      "Epoch 2/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6962 - accuracy: 0.5132 - val_loss: 0.6946 - val_accuracy: 0.5054\n",
      "Epoch 3/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6949 - accuracy: 0.5124 - val_loss: 0.6948 - val_accuracy: 0.5087\n",
      "Epoch 4/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6938 - accuracy: 0.5163 - val_loss: 0.7121 - val_accuracy: 0.5060\n",
      "Epoch 5/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6949 - accuracy: 0.5194 - val_loss: 0.6996 - val_accuracy: 0.4991\n",
      "Epoch 6/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6953 - accuracy: 0.5127 - val_loss: 0.6991 - val_accuracy: 0.5001\n",
      "Epoch 7/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6931 - accuracy: 0.5195 - val_loss: 0.6979 - val_accuracy: 0.5031\n",
      "Epoch 8/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6929 - accuracy: 0.5206 - val_loss: 0.6943 - val_accuracy: 0.5115\n",
      "Epoch 9/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6938 - accuracy: 0.5184 - val_loss: 0.6959 - val_accuracy: 0.4988\n",
      "Epoch 10/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6927 - accuracy: 0.5216 - val_loss: 0.6946 - val_accuracy: 0.5033\n",
      "Epoch 11/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6926 - accuracy: 0.5220 - val_loss: 0.6955 - val_accuracy: 0.5071\n",
      "Epoch 12/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6926 - accuracy: 0.5202 - val_loss: 0.6941 - val_accuracy: 0.5085\n",
      "Epoch 13/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6933 - accuracy: 0.5179 - val_loss: 0.7082 - val_accuracy: 0.5049\n",
      "Epoch 14/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6939 - accuracy: 0.5175 - val_loss: 0.7013 - val_accuracy: 0.5033\n",
      "Epoch 15/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6926 - accuracy: 0.5203 - val_loss: 0.6947 - val_accuracy: 0.5051\n",
      "Epoch 16/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6912 - accuracy: 0.5257 - val_loss: 0.6952 - val_accuracy: 0.5044\n",
      "Epoch 17/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6909 - accuracy: 0.5292 - val_loss: 0.6946 - val_accuracy: 0.5065\n",
      "Epoch 18/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6914 - accuracy: 0.5247 - val_loss: 0.6943 - val_accuracy: 0.5053\n",
      "Epoch 19/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6913 - accuracy: 0.5252 - val_loss: 0.6956 - val_accuracy: 0.4953\n",
      "Epoch 20/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6919 - accuracy: 0.5234 - val_loss: 0.6959 - val_accuracy: 0.5062\n",
      "Epoch 21/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6905 - accuracy: 0.5266 - val_loss: 0.6961 - val_accuracy: 0.5023\n",
      "Epoch 22/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6903 - accuracy: 0.5295 - val_loss: 0.6956 - val_accuracy: 0.4961\n",
      "Epoch 23/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6881 - accuracy: 0.5328 - val_loss: 0.6963 - val_accuracy: 0.4965\n",
      "Epoch 24/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6886 - accuracy: 0.5332 - val_loss: 0.6975 - val_accuracy: 0.4938\n",
      "Epoch 25/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6876 - accuracy: 0.5297 - val_loss: 0.7025 - val_accuracy: 0.5074\n",
      "Epoch 26/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6869 - accuracy: 0.5338 - val_loss: 0.6968 - val_accuracy: 0.4956\n",
      "Epoch 27/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6856 - accuracy: 0.5309 - val_loss: 0.6979 - val_accuracy: 0.4992\n",
      "Epoch 28/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6833 - accuracy: 0.5369 - val_loss: 0.6992 - val_accuracy: 0.5055\n",
      "Epoch 29/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6816 - accuracy: 0.5414 - val_loss: 0.7007 - val_accuracy: 0.4992\n",
      "Epoch 30/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6807 - accuracy: 0.5391 - val_loss: 0.7032 - val_accuracy: 0.5038\n",
      "Epoch 31/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6765 - accuracy: 0.5441 - val_loss: 0.7023 - val_accuracy: 0.5044\n",
      "Epoch 32/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6733 - accuracy: 0.5514 - val_loss: 0.7083 - val_accuracy: 0.5009\n",
      "Epoch 33/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6705 - accuracy: 0.5501 - val_loss: 0.7131 - val_accuracy: 0.4990\n",
      "Epoch 34/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6663 - accuracy: 0.5550 - val_loss: 0.7174 - val_accuracy: 0.4952\n",
      "Epoch 35/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6638 - accuracy: 0.5580 - val_loss: 0.7113 - val_accuracy: 0.5017\n",
      "Epoch 36/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6603 - accuracy: 0.5618 - val_loss: 0.7320 - val_accuracy: 0.5067\n",
      "Epoch 37/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6528 - accuracy: 0.5666 - val_loss: 0.7306 - val_accuracy: 0.5065\n",
      "Epoch 38/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6467 - accuracy: 0.5730 - val_loss: 0.7297 - val_accuracy: 0.5013\n",
      "Epoch 39/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6383 - accuracy: 0.5787 - val_loss: 0.7461 - val_accuracy: 0.4922\n",
      "Epoch 40/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6331 - accuracy: 0.5847 - val_loss: 0.7678 - val_accuracy: 0.4987\n",
      "Epoch 41/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6259 - accuracy: 0.5843 - val_loss: 0.7689 - val_accuracy: 0.5080\n",
      "Epoch 42/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6283 - accuracy: 0.5857 - val_loss: 0.7505 - val_accuracy: 0.5024\n",
      "[0.0006  0.01685 0.1599 ]\n",
      "Training with significance = 0.32, run 9\n",
      "Train on 40762 samples, validate on 10191 samples\n",
      "Epoch 1/1000\n",
      "40762/40762 [==============================] - 2s 44us/sample - loss: 0.7118 - accuracy: 0.5051 - val_loss: 0.6996 - val_accuracy: 0.5000\n",
      "Epoch 2/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6967 - accuracy: 0.5122 - val_loss: 0.6948 - val_accuracy: 0.5119\n",
      "Epoch 3/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6938 - accuracy: 0.5137 - val_loss: 0.6954 - val_accuracy: 0.5093\n",
      "Epoch 4/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6930 - accuracy: 0.5169 - val_loss: 0.6998 - val_accuracy: 0.5059\n",
      "Epoch 5/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6963 - accuracy: 0.5139 - val_loss: 0.6965 - val_accuracy: 0.5021\n",
      "Epoch 6/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6940 - accuracy: 0.5137 - val_loss: 0.6975 - val_accuracy: 0.4995\n",
      "Epoch 7/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6928 - accuracy: 0.5193 - val_loss: 0.6962 - val_accuracy: 0.4988\n",
      "Epoch 8/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6924 - accuracy: 0.5212 - val_loss: 0.6945 - val_accuracy: 0.5063\n",
      "Epoch 9/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6935 - accuracy: 0.5178 - val_loss: 0.6944 - val_accuracy: 0.5041\n",
      "Epoch 10/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6912 - accuracy: 0.5249 - val_loss: 0.6955 - val_accuracy: 0.5054\n",
      "Epoch 11/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6909 - accuracy: 0.5237 - val_loss: 0.6960 - val_accuracy: 0.5054\n",
      "Epoch 12/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6911 - accuracy: 0.5252 - val_loss: 0.6954 - val_accuracy: 0.5037\n",
      "Epoch 13/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6942 - accuracy: 0.5173 - val_loss: 0.6948 - val_accuracy: 0.5057\n",
      "Epoch 14/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6928 - accuracy: 0.5199 - val_loss: 0.7000 - val_accuracy: 0.5028\n",
      "Epoch 15/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6911 - accuracy: 0.5219 - val_loss: 0.6952 - val_accuracy: 0.4963\n",
      "Epoch 16/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6911 - accuracy: 0.5216 - val_loss: 0.6956 - val_accuracy: 0.5099\n",
      "Epoch 17/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6907 - accuracy: 0.5259 - val_loss: 0.6964 - val_accuracy: 0.5022\n",
      "Epoch 18/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6902 - accuracy: 0.5230 - val_loss: 0.6952 - val_accuracy: 0.5074\n",
      "Epoch 19/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6900 - accuracy: 0.5250 - val_loss: 0.6967 - val_accuracy: 0.5003\n",
      "Epoch 20/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6891 - accuracy: 0.5294 - val_loss: 0.6965 - val_accuracy: 0.4979\n",
      "Epoch 21/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6882 - accuracy: 0.5261 - val_loss: 0.6970 - val_accuracy: 0.5041\n",
      "Epoch 22/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6877 - accuracy: 0.5305 - val_loss: 0.6976 - val_accuracy: 0.4948\n",
      "Epoch 23/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6863 - accuracy: 0.5359 - val_loss: 0.6978 - val_accuracy: 0.4987\n",
      "Epoch 24/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6853 - accuracy: 0.5361 - val_loss: 0.6987 - val_accuracy: 0.4960\n",
      "Epoch 25/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6855 - accuracy: 0.5314 - val_loss: 0.6984 - val_accuracy: 0.5059\n",
      "Epoch 26/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6829 - accuracy: 0.5384 - val_loss: 0.7029 - val_accuracy: 0.4970\n",
      "Epoch 27/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6812 - accuracy: 0.5411 - val_loss: 0.7023 - val_accuracy: 0.4972\n",
      "Epoch 28/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6798 - accuracy: 0.5414 - val_loss: 0.7038 - val_accuracy: 0.5045\n",
      "Epoch 29/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6777 - accuracy: 0.5456 - val_loss: 0.7087 - val_accuracy: 0.4939\n",
      "Epoch 30/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6736 - accuracy: 0.5507 - val_loss: 0.7150 - val_accuracy: 0.5002\n",
      "Epoch 31/1000\n",
      "40762/40762 [==============================] - 1s 30us/sample - loss: 0.6699 - accuracy: 0.5507 - val_loss: 0.7153 - val_accuracy: 0.4948\n",
      "Epoch 32/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6635 - accuracy: 0.5631 - val_loss: 0.7280 - val_accuracy: 0.4957\n",
      "Epoch 33/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6625 - accuracy: 0.5602 - val_loss: 0.7247 - val_accuracy: 0.4944\n",
      "Epoch 34/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6570 - accuracy: 0.5656 - val_loss: 0.7229 - val_accuracy: 0.4993\n",
      "Epoch 35/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6519 - accuracy: 0.5701 - val_loss: 0.7396 - val_accuracy: 0.5026\n",
      "Epoch 36/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6502 - accuracy: 0.5703 - val_loss: 0.7468 - val_accuracy: 0.5011\n",
      "Epoch 37/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6391 - accuracy: 0.5782 - val_loss: 0.7632 - val_accuracy: 0.4977\n",
      "Epoch 38/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6366 - accuracy: 0.5802 - val_loss: 0.7658 - val_accuracy: 0.4946\n",
      "Epoch 39/1000\n",
      "40762/40762 [==============================] - 1s 29us/sample - loss: 0.6265 - accuracy: 0.5859 - val_loss: 0.7862 - val_accuracy: 0.5030\n",
      "[0.00035 0.01695 0.15115]\n",
      "Mean: of significance = 0.32 [0.0007425  0.01555083 0.1381119 ]\n",
      "Std of significance = 0.32:  [0.00050225 0.00751738 0.03006563]\n",
      "Training with significance = 0.63, run 0\n",
      "Train on 40815 samples, validate on 10204 samples\n",
      "Epoch 1/1000\n",
      "40815/40815 [==============================] - 2s 45us/sample - loss: 0.7130 - accuracy: 0.5034 - val_loss: 0.6951 - val_accuracy: 0.5092\n",
      "Epoch 2/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6962 - accuracy: 0.5095 - val_loss: 0.6963 - val_accuracy: 0.5012\n",
      "Epoch 3/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6944 - accuracy: 0.5117 - val_loss: 0.7079 - val_accuracy: 0.5145\n",
      "Epoch 4/1000\n",
      "40815/40815 [==============================] - 1s 30us/sample - loss: 0.6971 - accuracy: 0.5104 - val_loss: 0.6965 - val_accuracy: 0.5124\n",
      "Epoch 5/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6964 - accuracy: 0.5102 - val_loss: 0.6940 - val_accuracy: 0.5139\n",
      "Epoch 6/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6954 - accuracy: 0.5121 - val_loss: 0.7020 - val_accuracy: 0.5111\n",
      "Epoch 7/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6940 - accuracy: 0.5211 - val_loss: 0.6937 - val_accuracy: 0.5080\n",
      "Epoch 8/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6930 - accuracy: 0.5198 - val_loss: 0.7005 - val_accuracy: 0.5046\n",
      "Epoch 9/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6932 - accuracy: 0.5180 - val_loss: 0.6935 - val_accuracy: 0.5057\n",
      "Epoch 10/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6931 - accuracy: 0.5194 - val_loss: 0.6950 - val_accuracy: 0.5070\n",
      "Epoch 11/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6931 - accuracy: 0.5189 - val_loss: 0.6953 - val_accuracy: 0.5032\n",
      "Epoch 12/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6926 - accuracy: 0.5191 - val_loss: 0.6946 - val_accuracy: 0.5068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6922 - accuracy: 0.5206 - val_loss: 0.6956 - val_accuracy: 0.5003\n",
      "Epoch 14/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6937 - accuracy: 0.5225 - val_loss: 0.6949 - val_accuracy: 0.5052\n",
      "Epoch 15/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6918 - accuracy: 0.5204 - val_loss: 0.6936 - val_accuracy: 0.5117\n",
      "Epoch 16/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6916 - accuracy: 0.5238 - val_loss: 0.6982 - val_accuracy: 0.5031\n",
      "Epoch 17/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6914 - accuracy: 0.5217 - val_loss: 0.6954 - val_accuracy: 0.5017\n",
      "Epoch 18/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6915 - accuracy: 0.5228 - val_loss: 0.6936 - val_accuracy: 0.5154\n",
      "Epoch 19/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6900 - accuracy: 0.5263 - val_loss: 0.6985 - val_accuracy: 0.5044\n",
      "Epoch 20/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6910 - accuracy: 0.5241 - val_loss: 0.6944 - val_accuracy: 0.5097\n",
      "Epoch 21/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6893 - accuracy: 0.5340 - val_loss: 0.6965 - val_accuracy: 0.5005\n",
      "Epoch 22/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6908 - accuracy: 0.5276 - val_loss: 0.6965 - val_accuracy: 0.5095\n",
      "Epoch 23/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6902 - accuracy: 0.5258 - val_loss: 0.6953 - val_accuracy: 0.5093\n",
      "Epoch 24/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6904 - accuracy: 0.5280 - val_loss: 0.6963 - val_accuracy: 0.5030\n",
      "Epoch 25/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6882 - accuracy: 0.5269 - val_loss: 0.6975 - val_accuracy: 0.5067\n",
      "Epoch 26/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6859 - accuracy: 0.5336 - val_loss: 0.6968 - val_accuracy: 0.5060\n",
      "Epoch 27/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6853 - accuracy: 0.5387 - val_loss: 0.7089 - val_accuracy: 0.4996\n",
      "Epoch 28/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6873 - accuracy: 0.5314 - val_loss: 0.6999 - val_accuracy: 0.5025\n",
      "Epoch 29/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6854 - accuracy: 0.5352 - val_loss: 0.7016 - val_accuracy: 0.5020\n",
      "Epoch 30/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6822 - accuracy: 0.5397 - val_loss: 0.7059 - val_accuracy: 0.5017\n",
      "Epoch 31/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6799 - accuracy: 0.5418 - val_loss: 0.7044 - val_accuracy: 0.5008\n",
      "Epoch 32/1000\n",
      "40815/40815 [==============================] - 1s 28us/sample - loss: 0.6784 - accuracy: 0.5428 - val_loss: 0.7031 - val_accuracy: 0.5045\n",
      "Epoch 33/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6751 - accuracy: 0.5487 - val_loss: 0.7117 - val_accuracy: 0.4993\n",
      "Epoch 34/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6764 - accuracy: 0.5450 - val_loss: 0.7033 - val_accuracy: 0.5022\n",
      "Epoch 35/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6720 - accuracy: 0.5509 - val_loss: 0.7044 - val_accuracy: 0.5026\n",
      "Epoch 36/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6699 - accuracy: 0.5497 - val_loss: 0.7345 - val_accuracy: 0.4908\n",
      "Epoch 37/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6644 - accuracy: 0.5562 - val_loss: 0.7255 - val_accuracy: 0.5015\n",
      "Epoch 38/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6560 - accuracy: 0.5657 - val_loss: 0.7234 - val_accuracy: 0.5088\n",
      "Epoch 39/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6527 - accuracy: 0.5672 - val_loss: 0.7306 - val_accuracy: 0.5075\n",
      "[0.00075 0.01055 0.09055]\n",
      "Training with significance = 0.63, run 1\n",
      "Train on 40815 samples, validate on 10204 samples\n",
      "Epoch 1/1000\n",
      "40815/40815 [==============================] - 2s 44us/sample - loss: 0.7024 - accuracy: 0.5029 - val_loss: 0.6949 - val_accuracy: 0.5109\n",
      "Epoch 2/1000\n",
      "40815/40815 [==============================] - 1s 29us/sample - loss: 0.6955 - accuracy: 0.5111 - val_loss: 0.6957 - val_accuracy: 0.5047\n",
      "Epoch 3/1000\n",
      "22500/40815 [===============>..............] - ETA: 0s - loss: 0.6945 - accuracy: 0.5106"
     ]
    }
   ],
   "source": [
    "n_train_sr_bkg = 25000\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "mean_results = []\n",
    "std_results = []\n",
    "for p_1 in [25]:\n",
    "    bkg_sr_jet_1 = np.load(f'background_images_sr_{p_1}_jet_1.npy')\n",
    "    bkg_sr_jet_2 = np.load(f'background_images_sr_{p_1}_jet_2.npy')\n",
    "    \n",
    "    bkg_sb_jet_1 = np.load(f'background_images_sb_{p_1}_jet_1.npy')\n",
    "    bkg_sb_jet_2 = np.load(f'background_images_sb_{p_1}_jet_2.npy')\n",
    "    \n",
    "    sb_sr_bkg = np.shape(bkg_sb_jet_1)[0]/np.shape(bkg_sr_jet_2)[0]\n",
    "    n_train_sb_bkg = int(np.round(n_train_sr_bkg*sb_sr_bkg))\n",
    "    n_test_sr_bkg = 20000\n",
    "    #First jet, background, SR\n",
    "    bkg_train_sr_1 = bkg_sr_jet_1[0:n_train_sr_bkg]\n",
    "    bkg_test_1 = bkg_sr_jet_1[n_train_sr_bkg:n_train_sr_bkg+n_test_sr_bkg]\n",
    "    #Second jet, background, SR\n",
    "    bkg_train_sr_2 = bkg_sr_jet_2[0:n_train_sr_bkg]\n",
    "    bkg_test_2 = bkg_sr_jet_2[n_train_sr_bkg:n_train_sr_bkg+n_test_sr_bkg]\n",
    "    #First jet, background, SB\n",
    "    bkg_train_sb_1 = bkg_sb_jet_1[0:n_train_sb_bkg]\n",
    "    #Second jet, background, SB\n",
    "    bkg_train_sb_2 = bkg_sb_jet_2[0:n_train_sb_bkg]\n",
    "    \n",
    "    signal_sr_jet_1 = np.load(f'id10_sr_{p_1}_jet_1.npy')\n",
    "    signal_sr_jet_2 = np.load(f'id10_sr_{p_1}_jet_2.npy')\n",
    "    \n",
    "    signal_sb_jet_1 = np.load(f'id10_sb_{p_1}_jet_1.npy')\n",
    "    signal_sb_jet_2 = np.load(f'id10_sb_{p_1}_jet_2.npy')\n",
    "    \n",
    "    sb_sr_signal = np.shape(signal_sb_jet_1)[0]/np.shape(signal_sr_jet_1)[0]\n",
    "    for n_train_sr_signal in [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 1000, 1100, 1200]:\n",
    "        significance = np.sqrt(2*((n_train_sr_signal+25000)*np.log(n_train_sr_signal/25000 + 1) - n_train_sr_signal))\n",
    "        \n",
    "        n_train_sb_signal = int(np.round(n_train_sr_signal*sb_sr_signal))\n",
    "        n_test_sr_signal = 20000\n",
    "\n",
    "        #First jet, signal, SR\n",
    "        signal_train_sr_1 = signal_sr_jet_1[0:n_train_sr_signal]\n",
    "        signal_test_1 = signal_sr_jet_1[n_train_sr_signal:n_train_sr_signal+n_test_sr_signal]\n",
    "\n",
    "        #Second jet, signal, SR\n",
    "        signal_train_sr_2 = signal_sr_jet_2[0:n_train_sr_signal]\n",
    "        signal_test_2 = signal_sr_jet_2[n_train_sr_signal:n_train_sr_signal+n_test_sr_signal]\n",
    "\n",
    "        #First jet, signal, SB\n",
    "        signal_train_sb_1 = signal_sb_jet_1[0:n_train_sb_signal]\n",
    "\n",
    "        #Second jet, signal, SB\n",
    "        signal_train_sb_2 = signal_sb_jet_2[0:n_train_sb_signal]\n",
    "        \n",
    "        train_sr_1 = np.concatenate((signal_train_sr_1, bkg_train_sr_1))\n",
    "        train_sr_2 = np.concatenate((signal_train_sr_2, bkg_train_sr_2))\n",
    "\n",
    "        train_label_sr = np.ones(np.shape(train_sr_1)[0])\n",
    "        \n",
    "        train_sb_1 = np.concatenate((signal_train_sb_1, bkg_train_sb_1))\n",
    "        train_sb_2 = np.concatenate((signal_train_sb_2, bkg_train_sb_2))\n",
    "\n",
    "        train_label_sb = np.zeros(np.shape(train_sb_1)[0])\n",
    "        \n",
    "        test_label_signal = np.ones(np.shape(signal_test_1)[0])\n",
    "        test_label_bkg = np.zeros(np.shape(bkg_test_1)[0])\n",
    "        \n",
    "        x_train_1 = np.concatenate((train_sr_1, train_sb_1))\n",
    "        x_train_2 = np.concatenate((train_sr_2, train_sb_2))\n",
    "        y_train = np.concatenate((train_label_sr, train_label_sb))\n",
    "        \n",
    "        indices = rng.permutation(np.shape(y_train)[0])\n",
    "        \n",
    "        x_train_1 = x_train_1[indices]\n",
    "        x_train_2 = x_train_2[indices]\n",
    "        y_train = y_train[indices]\n",
    "        \n",
    "        x_test_1 = np.concatenate((signal_test_1, bkg_test_1))\n",
    "        x_test_2 = np.concatenate((signal_test_2, bkg_test_2))\n",
    "        y_test = np.concatenate((test_label_signal, test_label_bkg))\n",
    "        \n",
    "        x_train_1 = x_train_1.reshape((np.shape(x_train_1)[0], p_1, p_1, 1))\n",
    "        x_train_2 = x_train_2.reshape((np.shape(x_train_2)[0], p_1, p_1, 1))\n",
    "    \n",
    "        x_test_1 = x_test_1.reshape((np.shape(x_test_1)[0],p_1,p_1,1))\n",
    "        x_test_2 = x_test_2.reshape((np.shape(x_test_2)[0],p_1,p_1,1))\n",
    "        epsilon_results = []\n",
    "        for i in range(10):\n",
    "            print(f'Training with significance = {significance:.2f}, run {i}')\n",
    "            cwola = CWoLA(p_1)\n",
    "            loss_object = keras.losses.BinaryCrossentropy()\n",
    "            optimizer = keras.optimizers.Adam(learning_rate = 1e-4)\n",
    "            cwola.compile(loss = loss_object, optimizer = optimizer, metrics = ['accuracy'])\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience=30, restore_best_weights=True)\n",
    "            cwola.fit({'jet_1': x_train_1, 'jet_2': x_train_2}, y_train, validation_split = 0.2,\n",
    "              shuffle = True, batch_size = 500, callbacks = [early_stopping], epochs = 1000)  \n",
    "            x_predict = cwola.predict([x_test_1, x_test_2])\n",
    "            fpr, tpr, th = roc_curve(y_test, x_predict)\n",
    "            f = interpolate.interp1d(fpr, tpr)\n",
    "            epsilon_s = f([0.001, 0.01, 0.1])\n",
    "            print(epsilon_s)\n",
    "            epsilon_results.append(epsilon_s)\n",
    "            \n",
    "        epsilon_results = np.array(epsilon_results)\n",
    "        print(f'Mean: of significance = {significance:.2f}', np.mean(epsilon_results, axis = 0))\n",
    "        print(f'Std of significance = {significance:.2f}: ', np.std(epsilon_results, axis = 0))\n",
    "        mean_results.append(np.mean(epsilon_results, axis = 0))\n",
    "        std_results.append(np.std(epsilon_results, axis = 0))\n",
    "\n",
    "mean_results = np.array(mean_results)\n",
    "std_results = np.array(std_results)\n",
    "os.chdir(r'/home/manhducnmd/pp_dijet/Results_full')\n",
    "with open('cwola_mean_id10.npy','wb') as f:\n",
    "    np.save(f, mean_results)\n",
    "with open('cwola_std_id10.npy', 'wb') as g:\n",
    "    np.save(g, std_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "547f734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: of significance = 0.32 [0.00064175 0.010218   0.108738  ]\n",
      "Std of significance = 0.32:  [0.00025207 0.00261597 0.02780098]\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean: of significance = {significance:.2f}', np.mean(epsilon_results, axis = 0))\n",
    "print(f'Std of significance = {significance:.2f}: ', np.std(epsilon_results, axis = 0))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
